# Practical Recommendations for Multilingual ASR Deployment

Based on comprehensive evaluation of 4 ASR models across 4 languages with 16,000+ transcriptions.

---

## ğŸ¯ Mode Selection: LIDâ†’ASR vs Language-Hinted

### **When to Use LIDâ†’ASR Pipeline (Mode A)**

**Recommended for:**
- âœ… Unknown language scenarios (call centers, public services)
- âœ… Mixed-language environments
- âœ… User-uploaded content without metadata
- âœ… Exploratory/research applications

**Trade-offs:**
- âš ï¸ Slower: LID adds XX seconds overhead per file
- âš ï¸ Less accurate: LID errors cascade to transcription
- âš ï¸ Requires longer audio: LID accuracy improves with context

**Performance:**
```
From results/lid_accuracy_summary.csv:

Overall LID Accuracy: XX.X%
- Spanish: XX.X%
- French: XX.X%
- Hungarian: XX.X%
- Mongolian: XX.X%

Impact on WER:
- Correct LID: WER XX.X% (similar to Mode B)
- Incorrect LID: WER XX.X% (degraded)
- Net effect: +X.X% WER compared to Mode B
```

**Practical Tip:**
> Use LIDâ†’ASR when language is truly unknown, but require minimum 3-5 seconds of audio for reliable language detection.

---

### **When to Use Language-Hinted ASR (Mode B)**

**Recommended for:**
- âœ… Known language scenarios (most deployments)
- âœ… Metadata available (video captions, podcast transcripts)
- âœ… Region-specific applications (monolingual countries)
- âœ… Maximum accuracy requirements
- âœ… Real-time applications (lower latency)

**Trade-offs:**
- âœ… Faster: No LID overhead
- âœ… More accurate: No LID errors
- âŒ Requires language metadata

**Performance:**
```
From results/wer_cer_results_summary.csv:

Average WER by Model (Language-Hinted):
- Whisper-small: XX.X%
- OmniLingual CTC 300M: XX.X%
- OmniLingual CTC 1B: XX.X%
- OmniLingual LLM 1B: XX.X%

Speed (RTF):
- OmniLingual CTC 300M: 0.01-0.02 (fastest)
- Whisper on ES/FR: 0.50 (acceptable)
- Whisper on MN: 36.98 (unacceptable for real-time)
```

**Practical Tip:**
> Default to language-hinted mode when language is known or can be inferred from context (user settings, geolocation, content source).

---

## ğŸš€ Model Selection Recommendations

### **Production Deployment Decision Tree**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Do you know the language in advance?â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”
    â”‚             â”‚
   YES           NO
    â”‚             â”‚
    â–¼             â–¼
Language-    Use LIDâ†’ASR
Hinted       (Whisper)
    â”‚
    â”œâ”€â”€â”€ Real-time required? â”€â”€â”€â”€â”€â”€â”
    â”‚                              â”‚
   YES                            NO
    â”‚                              â”‚
    â–¼                              â–¼
    â”œâ”€ High-resource lang? â”€â”€â”  Accuracy critical?
    â”‚                        â”‚        â”‚
   YES                      NO    â”Œâ”€â”€â”€â”´â”€â”€â”€â”
    â”‚                        â”‚   YES     NO
    â–¼                        â–¼    â”‚       â”‚
OmniLingual              OmniL   â”‚       â”‚
CTC 300M                CTC 1B   â”‚       â”‚
(Fastest)              (Balanced)â”‚       â”‚
                                  â–¼       â–¼
                            OmniLingual  Whisper
                            LLM 1B      (if fast
                           (Best WER)   enough)
```

---

### **Specific Recommendations by Use Case**

#### **1. High-Volume Call Center (Known Language)**
**Recommendation:** OmniLingual CTC 300M + Language-Hinted

**Rationale:**
- Real-time processing (RTF 0.01-0.02)
- Acceptable accuracy (WER XX.X%)
- Consistent across languages
- Low GPU memory (~2GB)

**Expected Performance:**
- Latency: <100ms per utterance
- Throughput: 50+ concurrent streams
- Cost: Low compute requirements

---

#### **2. Multilingual Video Platform (Unknown Language)**
**Recommendation:** Whisper + LIDâ†’ASR â†’ OmniLingual for confirmed language

**Rationale:**
- Initial LID with Whisper (accurate but slow)
- Once language confirmed, switch to faster model
- Batch processing (not real-time)

**Expected Performance:**
- LID accuracy: XX.X%
- Processing: Offline/batch acceptable
- Cost: Higher compute, but not real-time

---

#### **3. Low-Resource Language Focus (e.g., Mongolian)**
**Recommendation:** OmniLingual CTC 1B or LLM 1B + Language-Hinted

**Rationale:**
- Whisper shows 74Ã— slowdown on Mongolian (RTF 36.98)
- OmniLingual maintains consistent speed
- Better low-resource language support

**Expected Performance:**
- WER: XX.X% (OmniLingual) vs XX.X% (Whisper)
- RTF: 0.05 (OmniLingual) vs 36.98 (Whisper)
- **OmniLingual enables real-time MN ASR, Whisper does not**

---

#### **4. Academic/Research Application**
**Recommendation:** Run multiple models + both modes

**Rationale:**
- Compare approaches
- Identify best for specific data
- Generate comprehensive results

**Setup:**
- Test both LIDâ†’ASR and language-hinted
- Evaluate all models
- Use this thesis as template! ğŸ“

---

#### **5. Mobile/Edge Deployment**
**Recommendation:** OmniLingual CTC 300M + Language-Hinted

**Rationale:**
- Small model size (~300M parameters)
- Low memory footprint
- Can run on device
- Consistent speed

**Expected Performance:**
- Model size: ~1.2GB on disk
- RAM: ~2GB
- Mobile GPU: Sufficient

---

## ğŸ”§ Implementation Best Practices

### **1. Audio Preprocessing**

**Recommended Pipeline:**
```python
1. Resample to 16kHz (model requirement)
2. Remove silence (energy-based VAD)
3. Normalize volume (-20dB LUFS target)
4. Chunk long audio (<30s segments)
```

**Why:**
- Models trained on 16kHz
- Silence removal improves speed
- Normalization improves accuracy
- Chunking prevents memory issues

---

### **2. Error Handling**

**LID Confidence Thresholding:**
```python
if lid_confidence < 0.7:
    # Retry with longer context
    # Or fall back to language-hinted with user selection
```

**WER Monitoring:**
```python
if wer > threshold:
    # Flag for human review
    # Possibly incorrect language or poor audio
```

---

### **3. Resource Management**

**Batching Strategy:**
```python
# For OmniLingual (GPU efficient)
batch_size = 16  # Process 16 audio files together

# For Whisper (sequential better)
batch_size = 1   # One at a time (model switching overhead)
```

**Memory Management:**
```python
# Offload models not in use
# Cache frequently used languages
# Monitor GPU memory usage
```

---

## ğŸ“ˆ Performance Optimization

### **Speed Optimization**

**For Mongolian (Whisper):**
- âŒ Whisper RTF 36.98 â†’ NOT real-time capable
- âœ… Switch to OmniLingual RTF 0.05 â†’ 700Ã— faster!

**General Tips:**
- Use GPU when available
- Batch processing for throughput
- Language-specific model selection
- Pre-load models in production

---

### **Accuracy Optimization**

**Pre-processing:**
- Remove background noise
- Normalize audio levels
- Ensure sufficient length (>3s for LID)

**Post-processing:**
- Language model rescoring
- Confidence-based filtering
- Custom vocabulary injection

---

## ğŸŒ Language-Specific Recommendations

### **Mongolian (Low-Resource, Cyrillic)**
- âœ… **Use:** OmniLingual CTC 1B or LLM 1B
- âœ… **Mode:** Language-hinted (LID less reliable)
- âŒ **Avoid:** Whisper (too slow)
- **WER:** XX.X% (best model)

### **Hungarian (Low-Resource, Latin)**
- âœ… **Use:** OmniLingual models
- âœ… **Mode:** Either (LID reasonably accurate)
- âš ï¸ **Whisper:** May be slow, test first
- **WER:** XX.X% (best model)

### **Spanish (High-Resource)**
- âœ… **Use:** Any model works well
- âœ… **Mode:** Either (excellent LID accuracy)
- âœ… **Whisper:** Fast enough (RTF 0.50)
- **WER:** XX.X% (best model)

### **French (High-Resource)**
- âœ… **Use:** Any model works well
- âš ï¸ **LID:** Can confuse with Spanish (XX% confusion rate)
- âœ… **Mode:** Language-hinted preferred for ES/FR confusion
- **WER:** XX.X% (best model)

---

## ğŸš§ Future Extensions

### **1. Improved LID for Short Clips**

**Current Limitation:**
- LID accuracy degrades on audio <3 seconds
- Common Voice median: ~X seconds

**Proposed Solutions:**

**A. Acoustic-Linguistic Fusion:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Audio Input   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”
    â”‚         â”‚
    â–¼         â–¼
Acoustic   Text-based
Features   Features
    â”‚         â”‚
    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
         â–¼
   Ensemble LID
   (Higher Accuracy)
```

**B. Context-Aware LID:**
- Use user history (previous language)
- Geolocation hints
- Content source metadata
- Bayesian prior combination

**C. Multi-Stage LID:**
1. Quick initial detection (first 1s)
2. Refinement with more context (3s)
3. Continuous verification (every 5s)

**Expected Improvement:**
- Target: XX% â†’ XX% accuracy on short clips
- Latency: <100ms additional overhead

---

### **2. Streaming ASR**

**Current Limitation:**
- Current implementation: Batch/offline processing
- Full audio required before transcription

**Proposed Streaming Architecture:**

```
Audio Stream (Real-Time)
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Audio Buffer   â”‚  â† 1-2 second chunks
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Streaming ASR  â”‚  â† Process chunks
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Partial Results â”‚  â† Emit continuously
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Implementation Steps:**

1. **Chunk-Based Processing:**
```python
chunk_size = 1.0  # seconds
overlap = 0.2     # seconds (for continuity)

for chunk in audio_stream:
    partial_result = model.transcribe(chunk)
    emit_partial(partial_result)
    update_context(partial_result)
```

2. **Context Carryover:**
- Maintain sliding window context
- Previous chunk influences next
- Smooth transitions between chunks

3. **End-of-Utterance Detection:**
- Silence detection
- Punctuation prediction
- Finalize transcription

**Expected Performance:**
- Latency: <500ms (chunk + processing)
- Accuracy: -X% compared to batch (acceptable)
- Use Case: Live captions, real-time translation

**Recommended Model:** OmniLingual CTC 300M (fastest, consistent)

---

### **3. Selective Fine-Tuning**

**Current Limitation:**
- General-purpose models
- May not excel on domain-specific vocabulary
- Fixed performance on niche domains

**Proposed Fine-Tuning Strategy:**

**A. Domain Adaptation:**
```
Pre-trained Model
    â”‚
    â–¼
Fine-tune on:
â”œâ”€ Medical terminology
â”œâ”€ Legal documents
â”œâ”€ Technical jargon
â””â”€ Regional accents
    â”‚
    â–¼
Domain-Specific Model
```

**B. Few-Shot Adaptation:**
- Collect 100-1000 domain samples
- Fine-tune last layers only
- Preserve multilingual capability

**C. Language-Specific Tuning:**
```
Focus on Low-Resource Languages:
â”œâ”€ Mongolian: +X hours training data
â”œâ”€ Hungarian: +X hours training data
â””â”€ Target: Reduce WER by XX%
```

**Implementation:**

```python
# Example: Fine-tune for medical Mongolian
base_model = load_model("omniASR_CTC_1B")

# Freeze early layers (preserve general knowledge)
for layer in base_model.layers[:-4]:
    layer.trainable = False

# Fine-tune on medical data
train_data = load_medical_mongolian_data()
fine_tuned_model = fine_tune(base_model, train_data, epochs=5)

# Expected: Medical term WER: XX% â†’ XX%
```

**Expected Improvements:**
- Domain-specific WER: -10-20% reduction
- General performance: Maintained
- Training time: 1-2 hours on single GPU

---

### **4. Multi-Modal ASR (Audio + Video)**

**Future Extension:**
- Lip-reading visual features
- Speaker diarization
- Gesture recognition
- Combined audio-visual model

**Expected Benefits:**
- Robustness to noise
- Speaker attribution
- Higher accuracy in challenging conditions

---

### **5. Code-Switching Support**

**Current Limitation:**
- Monolingual evaluation only
- No mixed-language support tested

**Proposed Approach:**

**A. Language-Switching Detection:**
```
Audio â†’ Segment-level LID â†’ Multi-language ASR â†’ Merged transcript
```

**B. Joint Code-Switch Model:**
- Train on code-switched data (SEAME, Miami Bangor)
- Single model handles both languages
- Smooth transitions

**Expected Use Cases:**
- Spanish-English (US)
- Mandarin-English (Singapore)
- Hindi-English (India)

---

### **6. Active Learning Loop**

**Continuous Improvement:**

```
Production ASR
    â”‚
    â–¼
Confidence-Based Sampling
    â”‚
    â–¼
Human Verification (Low Confidence)
    â”‚
    â–¼
Annotated Data
    â”‚
    â–¼
Model Fine-Tuning
    â”‚
    â””â”€â†’ Back to Production (Improved Model)
```

**Benefits:**
- Continuous accuracy improvement
- Domain adaptation over time
- Real-world data collection

---

## ğŸ“Š Summary Decision Matrix

| Scenario | Model | Mode | Expected WER | RTF |
|----------|-------|------|--------------|-----|
| **Known lang, real-time** | OmniLingual CTC 300M | Hinted | XX.X% | 0.01 |
| **Known lang, accuracy critical** | OmniLingual LLM 1B | Hinted | XX.X% | 0.5 |
| **Unknown lang, batch** | Whisper | LIDâ†’ASR | XX.X% | Variable |
| **Mongolian, any scenario** | OmniLingual (avoid Whisper) | Hinted | XX.X% | 0.05 |
| **Research/comparison** | All models | Both modes | - | - |

---

## âœ… Key Takeaways

1. **Default to language-hinted mode** when language is known (faster, more accurate)

2. **Use LIDâ†’ASR for unknown languages** but require >3s audio for reliability

3. **Avoid Whisper for Mongolian real-time** (74Ã— slower than Spanish)

4. **OmniLingual CTC 300M** is best for production deployment (fast + consistent)

5. **Future work priorities:**
   - Streaming ASR implementation
   - Short-clip LID improvement
   - Domain-specific fine-tuning

---

## ğŸ“ Questions to Consider Before Deployment

1. **Is language known in advance?** â†’ Affects mode selection
2. **Is real-time processing required?** â†’ Affects model selection
3. **What's the audio length distribution?** â†’ Affects LID reliability
4. **What's the accuracy requirement?** â†’ Affects model selection
5. **What hardware is available?** â†’ Affects feasibility

---

**This thesis provides a comprehensive foundation for making informed ASR deployment decisions!** ğŸ“âœ¨
