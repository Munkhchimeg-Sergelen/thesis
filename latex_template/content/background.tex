%----------------------------------------------------------------------------

\subsection{For Thesis}\label{for-thesis}

\subsubsection{Chapter 2: Background and Related Work}\label{chapter-2-background-and-related-work}

This chapter provides the foundational concepts necessary to understand the multilingual ASR evaluation presented in this thesis. We first introduce automatic speech recognition fundamentals (Section 2.1), then discuss the specific challenges of multilingual ASR systems (Section 2.2), cover language identification approaches (Section 2.3), and conclude with a review of related work (Section 2.4).

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection{2.1 Automatic Speech Recognition Fundamentals}\label{automatic-speech-recognition-fundamentals}

\subsubsection{2.1.1 Task Definition}\label{task-definition}

Automatic Speech Recognition (ASR) is the computational task of converting spoken language audio into written text. Formally, given an acoustic signal \(\mathbf{X} = (x_1, x_2, ..., x_T)\) representing an utterance, the goal is to produce the most likely word sequence \(\mathbf{W} = (w_1, w_2, ..., w_N)\) that corresponds to the spoken content.

In probabilistic terms, ASR seeks to find:

\[\hat{\mathbf{W}} = \arg\max_{\mathbf{W}} P(\mathbf{W} | \mathbf{X})\]

Using Bayes' rule, this can be decomposed into:

\[P(\mathbf{W} | \mathbf{X}) = \frac{P(\mathbf{X} | \mathbf{W}) P(\mathbf{W})}{P(\mathbf{X})}\]

where:
- \(P(\mathbf{X} | \mathbf{W})\) is the \textbf{acoustic model} (likelihood of audio given text)
- \(P(\mathbf{W})\) is the \textbf{language model} (prior probability of word sequence)
- \(P(\mathbf{X})\) is a normalization constant (can be ignored during maximization)

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsubsection{2.1.2 Neural ASR Architectures}\label{neural-asr-architectures}

Modern ASR systems are predominantly neural network-based, having largely replaced traditional Hidden Markov Model (HMM)-Gaussian Mixture Model (GMM) pipelines. Two major architectural paradigms dominate:

\paragraph{A. Encoder-Decoder (Sequence-to-Sequence) Models}\label{a.-encoder-decoder-sequence-to-sequence-models}

\textbf{Architecture}: An encoder processes the acoustic input into a sequence of hidden representations, which a decoder then converts into text via autoregressive generation.

\textbf{Key Components}:
- \textbf{Encoder}: Typically a stack of Transformer layers or convolutional neural networks (CNNs) that extract acoustic features
- \textbf{Decoder}: Autoregressive model (e.g., Transformer decoder) that generates text tokens sequentially
- \textbf{Attention Mechanism}: Allows the decoder to focus on relevant parts of the encoded audio

\textbf{Example}: OpenAI Whisper {[}Radford et al., 2022{]} uses a Transformer encoder-decoder architecture.

\textbf{Advantages}:
- Implicit language modeling (decoder learns linguistic structure)
- Strong performance on diverse languages and conditions
- Can handle tasks beyond transcription (translation, timestamps)

\textbf{Disadvantages}:
- Slower inference (sequential decoding)
- Higher computational cost
- Prone to hallucination (generating plausible but incorrect text)

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\paragraph{B. Connectionist Temporal Classification (CTC) Models}\label{b.-connectionist-temporal-classification-ctc-models}

\textbf{Architecture}: An encoder maps audio to frame-level character/phoneme predictions, with a special ``blank'' token allowing variable-length alignment.

\textbf{Key Innovation}: CTC introduces a many-to-one mapping where multiple encoder timesteps can map to the same output token or to ``blank'' (silence/non-speech). This eliminates the need for pre-aligned training data.

\textbf{Example}: Wav2Vec2 {[}Baevski et al., 2020{]} uses a CTC decoder atop a self-supervised encoder.

\textbf{Advantages}:
- Non-autoregressive (parallel decoding is possible)
- Faster inference
- Simpler training (no external language model required initially)

\textbf{Disadvantages}:
- Limited implicit language modeling (purely frame-wise predictions)
- Often requires external language model for competitive accuracy
- Less flexible (typically transcription-only)

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsubsection{2.1.3 Evaluation Metrics}\label{evaluation-metrics}

\paragraph{Word Error Rate (WER)}\label{word-error-rate-wer}

The standard metric for ASR evaluation, computed as the edit distance (Levenshtein distance) between hypothesis and reference at the word level:

\[\text{WER} = \frac{S + D + I}{N} \times 100\%\]

where \(S\) = substitutions, \(D\) = deletions, \(I\) = insertions, \(N\) = total reference words.

\textbf{Interpretation}: Lower is better. WER = 0\% indicates perfect transcription. WER can exceed 100\% if insertions are excessive.

\textbf{Limitations}:
- Insensitive to semantic equivalence (e.g., ``don't'' vs.~``do not'' counts as error)
- Word-based (problematic for morphologically rich languages)
- Requires normalized text (case, punctuation handling affects results)

\paragraph{Character Error Rate (CER)}\label{character-error-rate-cer}

Similar to WER but computed at the character level. More appropriate for:
- Morphologically rich languages (e.g., Hungarian, Turkish)
- Languages without clear word boundaries (e.g., Chinese, Japanese)
- Providing partial credit for near-correct words

\paragraph{Real-Time Factor (RTF)}\label{real-time-factor-rtf}

Measures inference speed relative to audio duration:

\[\text{RTF} = \frac{\text{Processing Time}}{\text{Audio Duration}}\]

\begin{itemize}
\tightlist
\item
  RTF \textless{} 1.0: Faster than real-time (suitable for live transcription)
\item
  RTF = 1.0: Real-time processing
\item
  RTF \textgreater{} 1.0: Slower than real-time (batch-only scenarios)
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection{2.2 Multilingual ASR Challenges}\label{multilingual-asr-challenges}

\subsubsection{2.2.1 Motivation for Multilingual Systems}\label{motivation-for-multilingual-systems}

\textbf{Language Diversity}: The world's \textasciitilde7,000 languages represent diverse phonological systems, writing scripts, and grammatical structures. Monolingual ASR systems are impractical at scale.

\textbf{Resource Inequality}: High-resource languages (English, Mandarin, Spanish) have abundant training data, while low-resource languages (Mongolian, Maori, many indigenous languages) lack sufficient data for monolingual training.

\textbf{Deployment Efficiency}: Multilingual models reduce deployment complexity by handling multiple languages with a single model, crucial for global applications (e.g., video subtitling, multilingual customer support).

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsubsection{2.2.2 Technical Challenges}\label{technical-challenges}

\paragraph{A. Phonetic and Phonological Variation}\label{a.-phonetic-and-phonological-variation}

Different languages use different sound inventories:
- \textbf{English}: \textasciitilde44 phonemes
- \textbf{Hawaiian}: \textasciitilde13 phonemes\\
- \textbf{!Xóõ (Khoisan language)}: \textasciitilde160 phonemes (including clicks)

A multilingual ASR system must learn representations that generalize across this diversity.

\paragraph{B. Orthographic Variation}\label{b.-orthographic-variation}

Writing systems vary widely:
- \textbf{Alphabetic}: Latin, Cyrillic, Arabic scripts
- \textbf{Logographic}: Chinese characters
- \textbf{Syllabic}: Japanese Kana, Cherokee syllabary
- \textbf{Abjad}: Hebrew, Arabic (vowels often omitted)

This complicates text tokenization and output representation.

\paragraph{C. Morphological Complexity}\label{c.-morphological-complexity}

Languages exhibit varying morphological typologies:
- \textbf{Analytic} (e.g., Mandarin): Minimal inflection, meaning conveyed by word order
- \textbf{Agglutinative} (e.g., Hungarian, Turkish): Extensive suffixing/prefixing
- \textbf{Fusional} (e.g., Spanish, Russian): Inflections encode multiple grammatical features

Word-based metrics like WER are less meaningful for highly agglutinative languages, where a single word might encode information equivalent to an English sentence.

\paragraph{D. Data Imbalance}\label{d.-data-imbalance}

Training data is highly skewed toward major languages:
- \textbf{English}: Tens of thousands of hours of transcribed speech
- \textbf{Low-resource languages}: Often fewer than 10 hours

Naive multilingual training can result in high-resource languages dominating the learned representations, degrading low-resource language performance.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsubsection{2.2.3 Multilingual Modeling Approaches}\label{multilingual-modeling-approaches}

\paragraph{A. Joint Multilingual Training}\label{a.-joint-multilingual-training}

Train a single model on pooled data from all languages.

\textbf{Advantages}:
- Cross-lingual transfer: Low-resource languages benefit from high-resource data
- Shared phonetic representations (e.g., /p/ sound is similar across languages)

\textbf{Challenges}:
- High-resource languages may dominate learning
- Negative transfer: Conflicting phoneme-grapheme mappings across languages

\textbf{Mitigation Strategies}:
- Data upsampling/downsampling for balanced language representation
- Language-specific output layers or adapters
- Multi-task learning with auxiliary language identification

\paragraph{B. Language-Specific Fine-Tuning}\label{b.-language-specific-fine-tuning}

Pre-train a multilingual model, then fine-tune separate models per language.

\textbf{Example}: Wav2Vec2-XLS-R {[}Babu et al., 2021{]} provides a pre-trained multilingual encoder, which is then fine-tuned with CTC heads for specific languages.

\textbf{Advantages}:
- Specialized models can achieve higher per-language accuracy
- Avoids negative transfer

\textbf{Disadvantages}:
- Increased deployment complexity (N models for N languages)
- Higher storage/memory requirements
- No cross-lingual generalization at inference time

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection{2.3 Language Identification for ASR}\label{language-identification-for-asr}

\subsubsection{2.3.1 Task Definition}\label{task-definition-1}

Language Identification (LID) is the task of determining which language is spoken in an audio segment. For multilingual ASR, LID serves two purposes:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Pre-processing}: Route audio to the appropriate language-specific ASR system
\item
  \textbf{Conditioning}: Inform a multilingual ASR system which language to expect
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsubsection{2.3.2 LID Approaches}\label{lid-approaches}

\paragraph{A. Standalone LID Models}\label{a.-standalone-lid-models}

Dedicated classifiers trained to predict language from acoustic features.

\textbf{Traditional Approach}: Extract acoustic features (MFCCs, i-vectors) and train classifiers (GMM, SVM).

\textbf{Neural Approach}: Train CNNs or RNNs on spectrograms to predict language labels.

\textbf{Advantages}:
- Decoupled from ASR (can optimize LID independently)
- Lightweight (small models suffice for LID)

\textbf{Disadvantages}:
- Requires separate LID training data
- Potential mismatch between LID and ASR acoustic processing

\paragraph{B. Joint LID-ASR Models}\label{b.-joint-lid-asr-models}

Integrate LID as an auxiliary task during ASR training.

\textbf{Example}: Whisper {[}Radford et al., 2022{]} includes LID as part of its encoder, predicting language from the same representations used for transcription.

\textbf{Advantages}:
- Shared representations (LID benefits from ASR training data)
- No separate LID model needed
- Consistent acoustic processing

\textbf{Disadvantages}:
- LID accuracy depends on ASR model quality
- May not be optimized for LID task specifically

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsubsection{2.3.3 LID Error Propagation}\label{lid-error-propagation}

In a LID→ASR pipeline, LID errors cascade into transcription errors:

\textbf{Case 1: Correct LID}

\begin{verbatim}
Audio (Spanish) → LID: Spanish ✓ → ASR(Spanish) → Correct transcript
\end{verbatim}

\textbf{Case 2: Incorrect LID}

\begin{verbatim}
Audio (Spanish) → LID: French ✗ → ASR(French) → Garbled transcript
\end{verbatim}

\textbf{Mitigation Strategies}:
1. \textbf{Confidence thresholding}: Only use LID prediction if confidence exceeds threshold; otherwise fall back to multilingual mode
2. \textbf{N-best LID}: Consider top-N language predictions and select ASR output with best internal score
3. \textbf{Code-switching handling}: Allow ASR to switch languages mid-utterance (advanced)

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection{2.4 Related Work}\label{related-work}

\subsubsection{2.4.1 OpenAI Whisper (2022)}\label{openai-whisper-2022}

\textbf{Key Innovation}: Large-scale weak supervision. Trained on 680,000 hours of multilingual audio scraped from the internet with noisy labels.

\textbf{Architecture}: Transformer encoder-decoder (74M to 1.5B parameters).

\textbf{Multilingual Approach}: Single model handles 99 languages via language-conditioned decoding.

\textbf{Findings} {[}Radford et al., 2022{]}:
- Competitive with state-of-the-art on English benchmarks (LibriSpeech)
- Strong zero-shot transfer to other languages
- Robust to acoustic variations (noise, accents, recording conditions)

\textbf{Limitations}:
- Hallucination: Can generate plausible but incorrect text, especially on silence or music
- Lower accuracy on low-resource languages compared to language-specific models
- Slower inference due to autoregressive decoding

\textbf{Relevance to This Work}: Whisper represents the \textbf{multilingual convenience} approach---one model handles all languages with minimal deployment complexity.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsubsection{2.4.2 Wav2Vec2-XLS-R (2021)}\label{wav2vec2-xls-r-2021}

\textbf{Key Innovation}: Self-supervised cross-lingual speech representation learning at scale.

\textbf{Architecture}: Convolutional encoder + Transformer layers, trained with masked prediction objective on 436,000 hours of unlabeled multilingual speech.

\textbf{Multilingual Approach}: Pre-trained multilingual encoder + language-specific CTC fine-tuning.

\textbf{Findings} {[}Babu et al., 2021{]}:
- Pre-training on multiple languages improves low-resource language performance via cross-lingual transfer
- Fine-tuned models achieve state-of-the-art results on BABEL and Common Voice benchmarks
- Self-supervised pre-training reduces labeled data requirements

\textbf{Limitations}:
- Requires fine-tuning for each target language (deployment complexity)
- CTC decoder limits flexibility (transcription-only, no language modeling)
- Pre-trained model is large (\textasciitilde300M parameters), and each fine-tuned model adds \textasciitilde300M more

\textbf{Relevance to This Work}: Wav2Vec2-XLS-R represents the \textbf{language-specific specialization} approach---higher per-language accuracy at the cost of deployment complexity.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsubsection{2.4.3 Comparative Studies}\label{comparative-studies}

\paragraph{Multilingual vs.~Monolingual Models}\label{multilingual-vs.-monolingual-models}

\textbf{{[}Pratap et al., 2020{]}}: Multilingual models benefit low-resource languages but may underperform monolinguals on high-resource languages.

\textbf{{[}Conneau et al., 2020{]}}: Cross-lingual transfer is most effective when pre-training languages share phonological or typological features.

\paragraph{LID Integration Strategies}\label{lid-integration-strategies}

\textbf{{[}Li et al., 2013{]}}: Explicit LID as a pre-processing step introduces latency and error propagation.

\textbf{{[}Watanabe et al., 2017{]}}: End-to-end multilingual models with implicit LID achieve comparable accuracy with lower latency.

\paragraph{Hardware and Deployment}\label{hardware-and-deployment}

\textbf{{[}Kim et al., 2021{]}}: Autoregressive models (encoder-decoder) have 3-5× higher latency than non-autoregressive (CTC) models on CPU, but the gap narrows on GPU.

\textbf{{[}Pratap et al., 2020{]}}: Quantization and model distillation can reduce multilingual model size by 4× with \textless5\% WER degradation.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsubsection{2.4.4 Research Gaps Addressed by This Thesis}\label{research-gaps-addressed-by-this-thesis}

Despite extensive prior work, several practical questions remain underexplored:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Multilingual Convenience vs.~Specialized Accuracy}: Most studies compare architectures within a single multilingual approach. Direct comparison of unified multilingual models (Whisper) against language-specific fine-tuned models (Wav2Vec2) on the \textbf{same} hardware and test set is limited.
\item
  \textbf{Model Scaling on Multilingual Tasks}: While Whisper paper reports accuracy across model sizes, the speed-accuracy trade-off for multilingual deployment (especially on consumer hardware) lacks detailed analysis.
\item
  \textbf{LID Impact Quantification}: The performance gap between oracle (language-hinted) and automatic (LID→ASR) modes is often mentioned but rarely rigorously quantified across diverse languages.
\item
  \textbf{Deployment-Oriented Evaluation}: Academic benchmarks focus on WER; practical deployment requires holistic evaluation including latency, memory, and coverage trade-offs.
\end{enumerate}

\textbf{This thesis addresses these gaps} by providing a controlled, reproducible comparison of multilingual ASR approaches with deployment-relevant metrics.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection{Key Takeaways}\label{key-takeaways}

✅ \textbf{ASR is a mature field} with neural methods now dominant\\
✅ \textbf{Multilingual ASR is essential} due to language diversity and resource inequality\\
✅ \textbf{Two paradigms exist}: Unified multilingual models vs.~language-specific specialization\\
✅ \textbf{LID integration} is critical but introduces complexity and potential errors\\
✅ \textbf{Prior work} has established strong baselines but leaves deployment trade-offs underexplored

\textbf{This thesis contributes} a rigorous, deployment-focused comparison of these approaches.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection{References}\label{references}

\begin{itemize}
\tightlist
\item
  Radford, A., et al.~(2022). ``Robust Speech Recognition via Large-Scale Weak Supervision.'' \emph{arXiv:2212.04356}.
\item
  Babu, A., et al.~(2021). ``XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale.'' \emph{arXiv:2111.09296}.
\item
  Baevski, A., et al.~(2020). ``wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations.'' \emph{NeurIPS 2020}.
\item
  Pratap, V., et al.~(2020). ``Massively Multilingual ASR: 50 Languages, 1 Model, 1 Billion Parameters.'' \emph{Interspeech 2020}.
\item
  Conneau, A., et al.~(2020). ``Unsupervised Cross-lingual Representation Learning for Speech Recognition.'' \emph{Interspeech 2020}.
\item
  Li, H., et al.~(2013). ``An Overview of Spoken Language Recognition: From GMM-UBM to Deep Neural Networks.'' \emph{arXiv:1303.1609}.
\item
  Watanabe, S., et al.~(2017). ``Language Independent End-to-End Architecture for Joint Language Identification and Speech Recognition.'' \emph{ASRU 2017}.
\item
  Kim, S., et al.~(2021). ``Comparison of Neural Speech Recognition Architectures for Edge Deployment.'' \emph{ICASSP 2021}.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection{TODO}\label{todo}

\begin{itemize}
\tightlist
\item[$\square$]
  Add full citations to bibliography
\item[$\square$]
  Verify all statistics (training hours, parameter counts)
\item[$\square$]
  Add figure: ASR architecture comparison diagram
\item[$\square$]
  Add figure: Multilingual training data distribution
\item[$\square$]
  Cross-reference with Methods chapter sections
\end{itemize}
