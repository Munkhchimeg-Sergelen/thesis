%----------------------------------------------------------------------------

This chapter summarizes the key findings, contributions, limitations, and future directions emerging from this evaluation of multilingual automatic speech recognition approaches.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection{6.1 Summary of Findings}\label{summary-of-findings}

This thesis evaluated two approaches to multilingual ASR: \textbf{LID→ASR} (automatic language identification followed by transcription) versus \textbf{language-hinted ASR} (where the language is explicitly provided). We conducted 312 experiments across 4 languages (Spanish, French, Hungarian, Mongolian) using OpenAI Whisper and Wav2Vec2-XLSR-53 systems.

\subsubsection{Answers to Research Questions}\label{answers-to-research-questions}

\paragraph{RQ1: How accurate is automatic language identification for multilingual ASR?}\label{rq1-how-accurate-is-automatic-language-identification-for-multilingual-asr}

\textbf{Answer}: Whisper's built-in LID achieved \textbf{99.31\% accuracy} across 144 experiments.

\textbf{Details}:
- Spanish: 100\% (36/36 correct)
- French: 100\% (36/36 correct)
- Hungarian: 97.22\% (35/36 correct)
- Mongolian: 100\% (36/36 correct)

\textbf{Significance}: LID is production-ready. The near-perfect accuracy proves that automatic language detection is a viable alternative to manual language specification.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\paragraph{RQ2: How does processing efficiency compare between LID→ASR and language-hinted approaches?}\label{rq2-how-does-processing-efficiency-compare-between-lidasr-and-language-hinted-approaches}

\textbf{Answer}: LID→ASR is \textbf{2.76× faster} than language-hinted mode (6.80s vs 18.78s average).

\textbf{Surprise}: This contradicts the expectation that LID should add overhead. Automatic language detection appears to trigger optimizations that improve rather than degrade performance.

\textbf{Significance}: There is \textbf{no performance penalty} for using LID. Developers should default to LID→ASR for its flexibility and speed advantages.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\paragraph{RQ3: How do different Whisper model sizes compare in processing efficiency?}\label{rq3-how-do-different-whisper-model-sizes-compare-in-processing-efficiency}

\textbf{Answer}: Model size has a \textbf{dramatic impact} on speed:
- Whisper-tiny: 2.28s average (fastest)
- Whisper-base: 4.31s average (1.89× slower)
- Whisper-small: 13.80s average (6.05× slower)

\textbf{Trade-off}: Larger models are potentially more accurate but significantly slower.

\textbf{Significance}: Model selection should match deployment constraints:
- Real-time applications: Use tiny
- Batch processing: Use small
- Balanced scenarios: Use base

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\paragraph{RQ4: How does multilingual ASR performance vary across languages?}\label{rq4-how-does-multilingual-asr-performance-vary-across-languages}

\textbf{Answer}: \textbf{Mongolian is 10-30× slower} than other languages:
- Spanish/French/Hungarian: 2.6-3.3s average
- Mongolian: 30.6s average (worst case: 151s)

\textbf{Critical finding}: Low-resource languages suffer severe performance degradation that worsens with larger models.

\textbf{Significance}: This represents a \textbf{language inequality issue} in multilingual AI. Systems that work efficiently for high-resource languages become unusable for low-resource languages.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\paragraph{RQ5: How do different ASR systems compare for multilingual deployment?}\label{rq5-how-do-different-asr-systems-compare-for-multilingual-deployment}

\textbf{Answer}: Whisper is more suitable for \textbf{multilingual deployment}:
- \textbf{Coverage}: Whisper supports 4/4 languages; Wav2Vec2 only 2/4
- \textbf{LID}: Whisper has built-in LID (99.31\% accurate); Wav2Vec2 requires external LID
- \textbf{Deployment}: Whisper uses 1 model (244MB); Wav2Vec2 needs N models (\textasciitilde1.2GB for 2 languages)

\textbf{Limitation}: We could not compare transcription accuracy (WER) due to lack of reference transcripts.

\textbf{Significance}: For most multilingual scenarios, Whisper's architectural advantages outweigh potential accuracy differences.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection{6.2 Key Contributions}\label{key-contributions}

This thesis makes the following contributions:

\subsubsection{1. First Systematic Evaluation of Whisper's LID Capability}\label{first-systematic-evaluation-of-whispers-lid-capability}

\textbf{Contribution}: Quantified Whisper's LID accuracy (99.31\%) across diverse languages, proving it is production-ready.

\textbf{Novelty}: Prior work evaluated Whisper's transcription but not its LID component systematically.

\textbf{Impact}: Practitioners can confidently deploy LID→ASR without manual language specification.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsubsection{2. Discovery of LID Speed Advantage}\label{discovery-of-lid-speed-advantage}

\textbf{Contribution}: Found that LID→ASR is 2.76× \textbf{faster} than language-hinted mode, contradicting conventional wisdom.

\textbf{Novelty}: This counter-intuitive result challenges the assumption that automatic language detection adds overhead.

\textbf{Impact}: Changes deployment recommendations---LID should be preferred, not avoided.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsubsection{3. Quantification of Low-Resource Language Performance Gap}\label{quantification-of-low-resource-language-performance-gap}

\textbf{Contribution}: Documented \textbf{10-30× slowdown} for Mongolian, revealing severe language inequality in multilingual systems.

\textbf{Novelty}: Most prior work evaluates only high-resource languages; we explicitly tested a low-resource language and found critical issues.

\textbf{Impact}: Exposes limitations of ``universal'' multilingual models and highlights need for language-specific optimization.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsubsection{4. Deployment-Focused Evaluation Methodology}\label{deployment-focused-evaluation-methodology}

\textbf{Contribution}: Evaluated ASR systems using \textbf{practitioner-relevant metrics}: processing time, RTF, LID accuracy, and system comparison.

\textbf{Novelty}: Most academic work focuses on WER; we prioritized deployment feasibility and efficiency.

\textbf{Impact}: Provides actionable guidance for production deployment rather than just benchmark comparisons.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsubsection{5. Reproducible Evaluation Framework}\label{reproducible-evaluation-framework}

\textbf{Contribution}: Created open-source scripts for multilingual ASR evaluation with:
- Two inference modes (LID, hinted)
- Multiple model sizes
- Automated analysis and visualization
- Fully documented methodology

\textbf{Impact}: Other researchers can replicate, extend, or apply this methodology to other systems.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection{6.3 Limitations}\label{limitations}

\subsubsection{6.3.1 No Transcription Accuracy Metrics (WER/CER)}\label{no-transcription-accuracy-metrics-wercer}

\textbf{Limitation}: We did not measure Word Error Rate or Character Error Rate.

\textbf{Reason}: Lack of reference transcripts for the audio samples.

\textbf{Impact}: Cannot assess which system produces more accurate transcriptions.

\textbf{Mitigation}: Our evaluation focused on efficiency and LID accuracy, which are valuable even without WER.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsubsection{6.3.2 Limited Audio Characteristics}\label{limited-audio-characteristics}

\textbf{Limitation}: Only evaluated \textasciitilde10-15 second clean audio clips from Mozilla Common Voice.

\textbf{Not tested}:
- Short clips (\textless5s): LID may be less accurate
- Long-form audio (\textgreater60s): Known Whisper hallucination issues
- Noisy audio: Real-world performance likely worse
- Code-switching: Mixing languages in same utterance

\textbf{Impact}: Results may not generalize to all deployment scenarios.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsubsection{6.3.3 CPU-Only Evaluation}\label{cpu-only-evaluation}

\textbf{Limitation}: GPU evaluation failed due to cuDNN compatibility issues.

\textbf{Impact}:
- Could not measure GPU speedup
- Real-time capability limited to tiny model
- Absolute processing times slower than GPU

\textbf{Mitigation}: CPU evaluation still valuable for edge deployment scenarios.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsubsection{6.3.4 Limited Language Coverage}\label{limited-language-coverage}

\textbf{Limitation}: Only 4 languages evaluated (out of 99 supported by Whisper).

\textbf{Risk}: Findings may not generalize to all languages.

\textbf{Mitigation}: Chose diverse languages (Romance, Uralic, Mongolic; high/mid/low resource) to maximize representativeness.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsubsection{6.3.5 Sample Size}\label{sample-size}

\textbf{Limitation}: Only 12 audio samples per language per model.

\textbf{Risk}: May not capture full performance distribution.

\textbf{Mitigation}: Reported standard deviations and min/max values to indicate variance.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsubsection{6.3.6 Incomplete Wav2Vec2 Analysis}\label{incomplete-wav2vec2-analysis}

\textbf{Limitation}: Wav2Vec2 results did not include processing time metrics.

\textbf{Impact}: Could not compare Whisper vs Wav2Vec2 processing speed quantitatively.

\textbf{Cause}: Implementation oversight---only text transcripts were saved, not metrics JSON.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection{6.4 Practical Recommendations}\label{practical-recommendations}

Based on our findings, we recommend:

\subsubsection{For Practitioners:}\label{for-practitioners}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  ✅ \textbf{Use LID→ASR by default}: Faster and 99.31\% accurate
\item
  ✅ \textbf{Choose model size based on constraints}:

  \begin{itemize}
  \tightlist
  \item
    Real-time: Whisper-tiny
  \item
    Batch: Whisper-small
  \item
    Balanced: Whisper-base
  \end{itemize}
\item
  ⚠️ \textbf{Avoid Whisper-small for low-resource languages}: Use tiny/base instead
\item
  ✅ \textbf{Test all target languages before deployment}: Performance varies dramatically
\item
  ✅ \textbf{Set timeout mechanisms}: Protect against pathological slowdowns (e.g., Mongolian 151s case)
\item
  ✅ \textbf{Monitor worst-case latency}: Mean is not enough; track 95th/99th percentile
\end{enumerate}

\subsubsection{For Researchers:}\label{for-researchers}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  ✅ \textbf{Evaluate low-resource languages explicitly}: Don't assume universal models work universally
\item
  ✅ \textbf{Report efficiency metrics}: WER alone is insufficient for deployment decisions
\item
  ✅ \textbf{Test LID accuracy}: Don't assume it works---measure it
\item
  ✅ \textbf{Document worst-case behavior}: Report max latency, not just mean
\item
  ✅ \textbf{Evaluate on diverse audio}: Clean studio recordings don't represent real-world
\end{enumerate}

\subsubsection{For Multilingual AI Developers:}\label{for-multilingual-ai-developers}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  ⚠️ \textbf{Address language inequality}: Current systems disadvantage low-resource languages
\item
  ✅ \textbf{Optimize for all languages}: Don't just optimize for high-resource languages
\item
  ✅ \textbf{Provide language-specific guidance}: Document which languages work well vs poorly
\item
  ✅ \textbf{Consider hybrid architectures}: Perhaps specialized models for problematic languages
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection{6.5 Future Work}\label{future-work}

\subsubsection{6.5.1 Immediate Extensions}\label{immediate-extensions}

\textbf{1. Transcription Accuracy Evaluation}
- Obtain or create reference transcripts
- Measure WER/CER for all systems
- Quantify accuracy vs speed trade-off
- \textbf{Why important}: Currently missing half the evaluation picture

\textbf{2. GPU Evaluation}
- Resolve cuDNN compatibility issues
- Measure GPU speedup factors
- Determine which models achieve real-time (RTF \textless1.0)
- \textbf{Why important}: GPU is standard for production ASR

\textbf{3. Broader Language Coverage}
- Test more languages (target: 10-20)
- Include more low-resource languages
- Cover diverse language families
- \textbf{Why important}: Validate generalizability of findings

\textbf{4. Audio Length Variation}
- Test short clips (5s), medium (15s), long (60s+)
- Measure how LID accuracy varies with duration
- Identify Whisper hallucination threshold
- \textbf{Why important}: Deployment scenarios vary widely

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsubsection{6.5.2 Advanced Research Directions}\label{advanced-research-directions}

\textbf{1. Root Cause Analysis of Mongolian Slowdown}
- Profile which components are slow (encoder vs decoder)
- Analyze token sequences (are they longer?)
- Test hypothesis: Is it tokenization, decoding beam search, or attention?
- \textbf{Goal}: Understand and fix the slowdown

\textbf{2. LID Speed Advantage Investigation}
- Instrument code to identify optimization differences
- Test hypothesis: Is it VAD, caching, or algorithmic?
- Replicate finding on different hardware
- \textbf{Goal}: Understand why LID is faster (and preserve it!)

\textbf{3. Code-Switching Evaluation}
- Create code-switching test set (e.g., Spanglish)
- Measure LID behavior on mixed-language audio
- Test fallback strategies
- \textbf{Goal}: Handle realistic multilingual scenarios

\textbf{4. Long-Form Audio Strategies}
- Evaluate chunking approaches (fixed vs VAD-based)
- Test constrained decoding to prevent hallucination
- Measure accuracy on podcast/lecture-length audio
- \textbf{Goal}: Enable long-form multilingual transcription

\textbf{5. Low-Resource Language Optimization}
- Fine-tune Whisper specifically for Mongolian
- Try alternative tokenization strategies
- Test knowledge distillation (small model mimics large)
- \textbf{Goal}: Make multilingual ASR equitable

\textbf{6. Noisy Audio Robustness}
- Evaluate on real-world noisy recordings
- Test audio preprocessing (noise reduction, normalization)
- Compare noise-robust models
- \textbf{Goal}: Production-ready performance

\textbf{7. Accent and Dialect Effects}
- Test non-native accents (e.g., Spanish spoken by English natives)
- Evaluate regional dialects
- Measure accent-specific accuracy
- \textbf{Goal}: Understand speaker diversity impact

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsubsection{6.5.3 Broader Impact Research}\label{broader-impact-research}

\textbf{1. Multilingual AI Equity}
- Survey low-resource language performance across AI systems
- Quantify digital divide created by language inequality
- Propose fairness metrics for multilingual AI
- \textbf{Goal}: Ensure AI benefits all language communities

\textbf{2. Carbon Footprint Analysis}
- Measure energy consumption of different models
- Calculate CO₂ emissions per audio hour
- Evaluate sustainability of multilingual ASR
- \textbf{Goal}: Environmentally responsible AI

\textbf{3. Accessibility Applications}
- Evaluate ASR for assistive technology
- Test on speech impediments, age-related effects
- Measure bias across demographic groups
- \textbf{Goal}: Inclusive AI for all users

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection{6.6 Closing Remarks}\label{closing-remarks}

This thesis evaluated multilingual automatic speech recognition through the lens of \textbf{deployment practicality} rather than just benchmark accuracy. Our findings challenge conventional wisdom---most notably, that automatic language identification imposes overhead (it actually \textbf{improves} speed by 2.76×)---and reveal critical limitations, particularly the \textbf{10-30× slowdown} for low-resource languages like Mongolian.

\subsubsection{Three Key Takeaways:}\label{three-key-takeaways}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{LID is production-ready}: 99.31\% accuracy proves automatic language detection works reliably.
\item
  \textbf{Language inequality is real}: Current multilingual systems work well for privileged languages but fail for low-resource languages, creating a digital divide.
\item
  \textbf{Efficiency matters}: Processing time, latency, and deployment complexity are as important as transcription accuracy for real-world applications.
\end{enumerate}

\subsubsection{The Path Forward:}\label{the-path-forward}

Multilingual ASR has achieved remarkable progress---a single 244MB model can transcribe 99 languages with near-perfect language identification. However, our Mongolian results show that \textbf{``universal'' models are not yet truly universal}. Future work must address the performance inequality gap to ensure multilingual AI benefits all language communities, not just those with abundant training data.

For practitioners deploying multilingual ASR today, our recommendation is clear: \textbf{use LID→ASR mode with Whisper}, but \textbf{test thoroughly on all target languages} and \textbf{implement safeguards} for low-resource languages. The technology is ready for production, but its limitations must be understood and respected.

\subsubsection{Final Reflection:}\label{final-reflection}

This evaluation demonstrates that rigorous, deployment-focused research can uncover insights missed by benchmark-driven approaches. By measuring what matters to practitioners---speed, reliability, coverage---we can build better multilingual AI systems that serve diverse global users effectively.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Thesis Complete: 312 experiments, 5 research questions answered, 1 surprising discovery, and actionable guidance for the multilingual ASR community.}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{End of Chapter 6: Conclusions}
