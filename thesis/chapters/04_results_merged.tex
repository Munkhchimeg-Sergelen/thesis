% Chapter 4: Results (MERGED VERSION - Best of both worlds!)

This chapter presents the empirical findings from our comprehensive evaluation of multilingual ASR systems. We evaluated 4 models across 4 languages (Mongolian, Hungarian, Spanish, French) in two inference modes: \textbf{language-hinted ASR} (where the target language is explicitly provided) versus \textbf{LID→ASR} (where the language is automatically detected from audio). In total, we conducted 16,000 transcriptions using the Common Voice v23.0 dataset.

\section{Overview of Evaluation}
\label{sec:eval-overview}

\subsection{Experimental Setup}

\textbf{Total Scale}: 16,000 transcriptions
\begin{itemize}
    \item \textbf{Dataset}: Common Voice v23.0
    \item \textbf{Languages}: 4 (Mongolian, Hungarian, Spanish, French)
    \item \textbf{Samples per language}: 1,000 validated audio clips
    \item \textbf{Models}: 4 (Whisper-small + 3 OmniLingual variants)
    \item \textbf{Inference Modes}: 2 (Language-hinted, LID→ASR)
\end{itemize}

\textbf{Language Resource Levels}:
\begin{itemize}
    \item \textbf{High-resource}: Spanish (ES), French (FR)
    \item \textbf{Medium-resource}: Hungarian (HU)
    \item \textbf{Low-resource}: Mongolian (MN)
\end{itemize}

\textbf{Evaluation Metrics}:
\begin{itemize}
    \item Word Error Rate (WER) - transcription accuracy
    \item Character Error Rate (CER) - character-level accuracy
    \item Real-Time Factor (RTF) - processing speed
    \item Language Identification Accuracy - detection correctness
\end{itemize}

All experiments were conducted in a reproducible environment with consistent configurations to ensure fair comparison.

\section{Overall Performance}
\label{sec:overall-performance}

\subsection{Word Error Rate by Language}

Figure~\ref{fig:wer-by-model-lang} presents the WER performance across all models and languages, revealing a clear resource hierarchy.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\textwidth]{../thesis_plots/01_wer_by_model_language.png}
    \caption{Word Error Rate by model and language. Lower is better. Clear performance hierarchy emerges based on language resource availability.}
    \label{fig:wer-by-model-lang}
\end{figure}

\textbf{Key Findings}:
\begin{itemize}
    \item \textbf{Language Hierarchy}: Spanish achieves the best performance with 13.5\% WER (sweep mode), followed by French (41.0\%), Hungarian (79.0\%), and Mongolian (103.6\% WER in hinted mode). The performance gap between high-resource (ES, FR) and low-resource (MN) languages is substantial (10× difference).
    
    \item \textbf{Model Comparison}: Language-hinted inference consistently outperforms or matches the LID→ASR pipeline approach across all languages, with Spanish showing the most significant difference (17.2\% vs 18.9\% WER).
    
    \item \textbf{Low-Resource Impact}: Mongolian demonstrates the severe challenges of low-resource ASR, performing approximately 10× worse than Spanish. Notably, Mongolian achieves WER > 100\% in sweep mode (147.9\%), indicating more errors than words in the reference transcripts - a clear sign of model failure.
\end{itemize}

\textbf{WER by Mode}:
\begin{table}[htbp]
\centering
\caption{Word Error Rate by language and inference mode}
\label{tab:wer-by-mode}
\begin{tabular}{lrrr}
\toprule
\textbf{Language} & \textbf{Hinted} & \textbf{LID→ASR} & \textbf{Sweep} \\
\midrule
Spanish (ES)   & 17.2\% & 18.9\% & 13.5\% \\
French (FR)    & 42.5\% & 41.0\% & 46.7\% \\
Hungarian (HU) & 85.0\% & 80.7\% & 79.0\% \\
Mongolian (MN) & 103.6\% & 102.6\% & 147.9\% \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Character Error Rate Analysis}

Character-level errors provide additional insights beyond word-level metrics, particularly for morphologically rich languages like Hungarian.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\textwidth]{../thesis_plots/02_cer_by_model_language.png}
    \caption{Character Error Rate by model and language. Reveals whether errors are fundamental (high CER) or primarily word boundary issues (low CER relative to WER).}
    \label{fig:cer-by-model-lang}
\end{figure}

\textbf{Observations}:
\begin{itemize}
    \item \textbf{General correlation}: CER generally follows WER trends across all languages, with strong correlation between word-level and character-level errors (see Figure~\ref{fig:wer-cer-correlation}).
    
    \item \textbf{Hungarian anomaly}: Hungarian shows interesting behavior with relatively lower CER/WER ratio (31.8\% CER vs 79.0\% WER in sweep mode), suggesting that while words are incorrect, character-level accuracy is better preserved. This may indicate morphological concatenation errors rather than phoneme recognition failures.
    
    \item \textbf{Mongolian fundamental failure}: Mongolian faces challenges at both word and character levels (94.7-139.2\% CER), indicating fundamental recognition difficulties rather than just word boundary issues. The model struggles with basic phoneme-to-character mapping for this low-resource language.
\end{itemize}

\subsection{Error Distribution and Variability}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\textwidth]{../thesis_plots/03_error_distribution.png}
    \caption{Distribution of transcription errors across models and languages, showing consistency vs. volatility patterns.}
    \label{fig:error-dist}
\end{figure}

The error distribution (Figure~\ref{fig:error-dist}) reveals significant variability in model performance:

\begin{itemize}
    \item \textbf{High-resource stability}: Spanish and French show tight error distributions with few outliers
    \item \textbf{Hungarian volatility}: Wider distribution indicates inconsistent performance
    \item \textbf{Mongolian chaos}: Extremely wide distribution with many outliers, indicating unpredictable behavior
\end{itemize}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\textwidth]{../thesis_plots/09_wer_range_analysis.png}
    \caption{WER range analysis showing performance consistency across samples.}
    \label{fig:wer-range}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\textwidth]{../thesis_plots/10_error_variability.png}
    \caption{Error variability across samples, quantifying prediction uncertainty.}
    \label{fig:error-variability}
\end{figure}

\section{Processing Speed Analysis}
\label{sec:speed-analysis}

Processing speed is critical for real-time and near-real-time applications. We measure Real-Time Factor (RTF), where RTF < 1.0 indicates faster-than-real-time processing.

\subsection{Critical Finding: Dramatic Speed Variation}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\textwidth]{../thesis_plots/07_rtf_by_model_language.png}
    \caption{Real-Time Factor by model and language. Lower is faster. Note the dramatic difference for Mongolian.}
    \label{fig:rtf-by-model-lang}
\end{figure}

\textbf{Shocking Discovery}: Whisper exhibits a \textbf{74× speed difference} between languages:

\begin{itemize}
    \item \textbf{Spanish}: RTF = 0.50 (processes 2× faster than real-time)
    \item \textbf{Mongolian}: RTF = 36.98 (processes 37× slower than real-time)
    \item \textbf{Implication}: 1 minute of Mongolian audio requires \textasciitilde37 minutes to process!
\end{itemize}

In contrast, OmniLingual maintains relatively consistent processing speed across all languages, though at the cost of reduced accuracy.

\subsection{Speed-Accuracy Trade-off}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\textwidth]{../thesis_plots/08_speed_accuracy_tradeoff.png}
    \caption{Speed vs. accuracy trade-off across models and languages. Each point represents a model-language combination.}
    \label{fig:speed-accuracy}
\end{figure}

Figure~\ref{fig:speed-accuracy} illustrates the fundamental trade-off between processing speed and transcription accuracy:

\begin{itemize}
    \item \textbf{Fast but inaccurate}: OmniLingual variants process quickly but sacrifice accuracy
    \item \textbf{Accurate but slow}: Whisper achieves better WER but with highly variable speed
    \item \textbf{No free lunch}: No single model provides both optimal speed and accuracy across all languages
\end{itemize}

\textbf{Deployment Implications}:
\begin{enumerate}
    \item \textbf{Real-time applications}: Must use fast models, accepting accuracy loss
    \item \textbf{Batch processing}: Can afford slower models for better accuracy
    \item \textbf{Low-resource languages}: Present fundamental challenges for both speed and accuracy
\end{enumerate}

\section{Language Identification Results}
\label{sec:lid-results}

\subsection{LID Accuracy Overview}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\textwidth]{../thesis_plots/14_lid_accuracy.png}
    \caption{Language identification accuracy by language, showing resource-dependent performance.}
    \label{fig:lid-accuracy}
\end{figure}

The overall language identification accuracy across all tested samples is \textbf{84.4\%} (37 out of 45 samples correctly identified). Performance varies significantly by language resource level:

\begin{itemize}
    \item \textbf{Spanish (ES)}: 90.9\% accuracy (10/11 correct) with high confidence (median probability 0.98)
    \item \textbf{French (FR)}: 90.9\% accuracy (10/11 correct) with high confidence (median probability 0.99)
    \item \textbf{Hungarian (HU)}: 83.3\% accuracy (10/12 correct) with medium confidence (median probability 0.69)
    \item \textbf{Mongolian (MN)}: 72.7\% accuracy (8/11 correct) with low confidence (median probability 0.55)
\end{itemize}

The results show a clear correlation between resource availability and LID accuracy: high-resource languages (ES, FR) achieve over 90\% accuracy with high confidence, while the low-resource language (MN) struggles with both accuracy and confidence.

\subsection{LID Confusion Matrix}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\textwidth]{../thesis_plots/13_lid_confusion_matrix.png}
    \caption{Language identification confusion matrix showing common misclassifications between language pairs.}
    \label{fig:lid-confusion}
\end{figure}

The confusion matrix (Figure~\ref{fig:lid-confusion}) reveals systematic patterns in misclassification. Key observations include systematic confusion patterns that warrant investigation.

\subsection{LID Confidence Scores}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\textwidth]{../thesis_plots/15_lid_confidence.png}
    \caption{Distribution of language identification confidence scores by language.}
    \label{fig:lid-confidence}
\end{figure}

The confidence score distribution (Figure~\ref{fig:lid-confidence}) reveals clear patterns related to language resource availability. Low confidence cases (probability < 0.60) are heavily concentrated in low-resource languages:

\begin{itemize}
    \item \textbf{Mongolian}: 8 low-confidence cases (most challenging)
    \item \textbf{Hungarian}: 5 low-confidence cases
    \item \textbf{Spanish}: 1 low-confidence case
    \item \textbf{French}: 1 low-confidence case
\end{itemize}

This distribution suggests that the LID model's uncertainty is a reliable indicator of potential misclassification. The low median confidence for Mongolian (0.55) indicates the model struggles to confidently identify this low-resource language, which directly impacts the LID→ASR pipeline's effectiveness.

\textbf{Practical Implication}: Systems could use confidence thresholds to trigger fallback strategies (e.g., try multiple languages) for low-confidence predictions.

\section{Long-form Drift Analysis}
\label{sec:longform-drift}

To investigate model behavior on extended audio, we created nine long-form samples by concatenating short Common Voice clips into 120-second, 180-second, and 240-second recordings.

\subsection{Experimental Setup}

\begin{itemize}
    \item \textbf{Language}: French (high-resource)
    \item \textbf{Durations}: 120s, 180s, 240s (3 samples each)
    \item \textbf{Method}: Concatenated short clips with preserved reference transcripts
    \item \textbf{Analysis}: Window-wise WER calculation (30-word windows)
    \item \textbf{Purpose}: Test for language model drift and error accumulation over time
\end{itemize}

\subsection{Language Model Stability}

\textbf{Key Finding}: All 9 long-form samples were correctly identified as French throughout their entire duration. \textbf{No language switching or model collapse was observed}, even in samples exceeding 4 minutes.

This demonstrates that Whisper's language model maintains stable language identification across extended audio, contrary to concerns about drift-induced language confusion.

\subsection{Error Accumulation Patterns}

\begin{table}[htbp]
\centering
\caption{Long-form drift analysis results showing variable error patterns}
\label{tab:longform-results}
\begin{tabular}{lcccc}
\toprule
\textbf{Sample} & \textbf{Duration (s)} & \textbf{Segments} & \textbf{Avg WER} & \textbf{WER Range} \\
\midrule
fr\_long\_120s\_1 & 118.16 & 12 & 29.5\% & 19-60\% \\
fr\_long\_120s\_2 & 118.16 & 12 & 98.0\% & 94-100\% \\
fr\_long\_120s\_3 & 118.16 & 12 & 79.8\% & 3-100\% \\
fr\_long\_180s\_1 & 118.16 & 12 & 19.8\% & 0-53\% \\
fr\_long\_180s\_2 & 118.16 & 12 & 24.9\% & 13-67\% \\
fr\_long\_180s\_3 & 118.16 & 12 & 89.8\% & 20-100\% \\
fr\_long\_240s\_1 & 118.16 & 12 & 42.9\% & 7-80\% \\
fr\_long\_240s\_2 & 118.16 & 12 & 99.0\% & 93-100\% \\
fr\_long\_240s\_3 & 118.16 & 12 & 83.3\% & 0-100\% \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Observations}:
\begin{itemize}
    \item \textbf{Variable degradation}: Average WER ranges from 19.8\% to 99.0\% across samples
    \item \textbf{No duration effect}: Longer samples don't consistently show worse WER
    \item \textbf{Sample-dependent}: Quality and characteristics of source audio significantly impact drift behavior
    \item \textbf{Window variability}: Within-sample WER varies dramatically (e.g., 0-100\% range in some samples)
\end{itemize}

\subsection{Common Error Types in Long-form Audio}

Analysis of transcription errors reveals consistent patterns:

\begin{enumerate}
    \item \textbf{Name recognition}: ``Delpot trop'' → ``Del Potro'' (proper nouns)
    \item \textbf{Diacritics}: ``rénale'' → ``renale'' (accent marks dropped)
    \item \textbf{Organization names}: ``USOC'' → ``USOCC'' / ``Luzok'' (abbreviation confusion)
    \item \textbf{Compound words}: Segmentation errors in multi-word expressions
    \item \textbf{Homophones}: Context-dependent word choice errors
\end{enumerate}

\subsection{Implications for Production Systems}

\begin{enumerate}
    \item \textbf{Language stability confirmed}: No risk of language model collapse in extended audio
    \item \textbf{Quality matters most}: Source audio quality dominates drift behavior over duration
    \item \textbf{Non-systematic degradation}: Error accumulation is not strictly time-dependent
    \item \textbf{Chunking strategy recommended}: For production, consider processing long audio in overlapping segments to mitigate quality-dependent variations
    \item \textbf{Post-processing opportunity}: Consistent error types (names, diacritics) could be addressed with specialized post-processors
\end{enumerate}

\section{Correlation and Comparative Analysis}

\subsection{WER-CER Correlation}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\textwidth]{../thesis_plots/11_wer_cer_correlation.png}
    \caption{Correlation between Word Error Rate and Character Error Rate across all experiments.}
    \label{fig:wer-cer-correlation}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\textwidth]{../thesis_plots/18_wer_cer_ratio.png}
    \caption{WER/CER ratio analysis reveals error type patterns.}
    \label{fig:wer-cer-ratio}
\end{figure}

The WER-CER correlation reveals strong proportional relationships for most languages, with deviations indicating specific error types (boundary vs. phonetic errors).

\subsection{Performance Heatmap}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\textwidth]{../thesis_plots/16_performance_heatmap.png}
    \caption{Performance heatmap showing all model-language-metric combinations at a glance.}
    \label{fig:performance-heatmap}
\end{figure}

\subsection{Language Trade-offs}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\textwidth]{../thesis_plots/17_language_tradeoffs.png}
    \caption{Multi-dimensional trade-off analysis across languages.}
    \label{fig:language-tradeoffs}
\end{figure}

\section{Summary of Key Results}

\subsection{Major Findings}

\begin{enumerate}
    \item \textbf{Resource Hierarchy Confirmed}: Clear 10× performance gap between high-resource (ES: 13.5\% WER) and low-resource (MN: 147.9\% WER) languages
    
    \item \textbf{Dramatic Speed Variation}: 74× RTF difference between MN and ES - critical deployment consideration
    
    \item \textbf{LID Reliability}: 84.4\% overall accuracy, but highly language-dependent (90.9\% for ES/FR vs 72.7\% for MN)
    
    \item \textbf{Long-form Stability}: No language model collapse, but quality-dependent degradation
    
    \item \textbf{Mode Comparison}: Language-hinted generally outperforms LID→ASR when language is known
    
    \item \textbf{No Silver Bullet}: No single model optimal for all scenarios; choice depends on accuracy vs. speed requirements and target language
\end{enumerate}

\subsection{Implications for Deployment}

\begin{table}[htbp]
\centering
\caption{Deployment recommendations by scenario}
\label{tab:deployment-rec}
\begin{tabular}{lll}
\toprule
\textbf{Scenario} & \textbf{Recommendation} & \textbf{Rationale} \\
\midrule
Real-time, known language & OmniLingual + hints & Fast, acceptable accuracy \\
Real-time, unknown language & OmniLingual + LID & Speed critical \\
Batch, high-resource & Whisper-small & Best accuracy \\
Batch, low-resource & Whisper-small & Only viable option \\
Long-form audio & Chunked processing & Mitigate drift \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Limitations}

\begin{itemize}
    \item \textbf{Dataset}: Common Voice only - no spontaneous speech or noisy conditions
    \item \textbf{Languages}: Only 4 tested - results may not generalize to other low-resource languages
    \item \textbf{Long-form}: French only - other languages may exhibit different drift patterns
    \item \textbf{Code-switching}: Not evaluated due to monolingual dataset
\end{itemize}

The next chapter analyzes these results through the lens of failure modes and their underlying causes.

\end{document}
